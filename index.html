<html>
<head>
<title>ASU Profolio</title>
<link href='http://fonts.googleapis.com/css?family=Nunito:300|Crimson+Text|Droid+Sans+Mono' rel='stylesheet' type='text/css'>
<link rel="stylesheet" title="Default" href="styles/github.css">
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script>  

<link rel="stylesheet" href="highlighting/styles/default.css">
<script src="highlighting/highlight.pack.js"></script>

<style type="text/css">
body {
	margin: 0px;
	width: 100%;
	font-family: 'Nunito', serif;
	font-size: 20px;
	background: #fcfcfc;
}
h1 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 36px;
	margin: 25px 0px 0px 0px;

}

h2 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 32px;
	margin: 15px 0px 35px 0px;
	color: #333;	
	word-spacing: 3px;
}

h3 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 26px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}
h4 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 22px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}

h5 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 18px;
	margin: 10px 0px 10px 0px;
	color: #111;
	word-spacing: 2px;
}

p, li {
	color: #444;
}

a {
	color: #DE3737;
}

.container {
	margin: 0px auto 0px auto;
	width: 1160px;
}

#header {
	background: #333;
	width: 100%;
}

#headersub {
	color: #ccc;
	width: 960px;
	margin: 0px auto 0px auto;
	padding: 20px 0px 20px 0px;
}

.chart {
	width: 480px;
}
.lol {
	font-size: 16px;
	color: #888;
	font-style: italic;
}
.sep {
	height: 1px;
	width: 100%;
	background: #999;
	margin: 20px 0px 20px 0px;
}
.footer{
	font-size: 16px;
}
.latex {
	width: 100%;
}

.latex img {
	display: block;
	margin: 0px auto 0px auto;
}

pre {
	font-family: 'Nunito';
	font-size: 14px;
}

table td {
  text-align: center;
  vertical-align: middle;
}

table td img {
  text-align: center;
  vertical-align: middle;
}

#contents a {
}
</style>
<script type="text/javascript">
    hljs.initHighlightingOnLoad();
</script>
</head>
<body>
<div id="header">
<div id="headersub">
<center>
<h1><b>Wearable Cyber-Physical System for Dance Performance<br>Porfolio by Jiqing Wen<span style="color: #DE3737"></b></h1>
</center>
</div>
</div>
<div class="container">

<div style="float: right; padding: 20px">
</div>
<p>Interactive dance is the combination of art and technology. In interactive dance performance, the stage effects are automatically generated or controlled based on stage cues such as the dancer's movement. As a dancer and researcher, I devoted my study to the idea of building an interactive dance system and creating novel works with such combination:</p>
<ul style="list-style-type:disc;">
  <li>BLE Beacon-based Interaction</li>
  <li>Accelerometer-based Interaction</li>
</ul>
<div style="clear:both">
<h2><b>BLE Beacon-based Interaction</b></h2>
<p>In this project, the system leverages a body area network, which is implicitly formed by four BLE beacons attached to a dancer's body parts, to monitor the dancer's performance and trigger visual and sound effects accordingly. A mobile device is used as a gateway to measure the signal strength received from the beacon nodes. The measurement is thus denoted as RSS, which is processed to monitor a dancer's relative distances during dance performance. </p>
<center>
<img src="image/Pic1.png" width="800px">
<p>Fig. 1: The body area network design and the mobile device. </p>
</center>
<p>Based on an empirical analysis that the RSS decreases with increasing distance, the system monitors two kinds of distance information: the distance between the dancer and the audience, and the distance between the dancer’s body parts. Instead of computing the exact distance, we estimate the relative distance based on the RSS data of the four beacons, and divide the estimated results into several classes. In this project, multinomial logistic regression is used to perform the classification. We view the four beacon signals to form a data vector, and learn a weight for each of the three classes. The predicted class probability for each class <img src="http://latex.codecogs.com/gif.latex? i" />, is the exponential of <img src="http://latex.codecogs.com/gif.latex? \textbf{w}^{T}_{i}\textbf{z} + \textbf{b}_{i}" />, normalized by the sum of probabilities of all classes. We use the cross-entropy loss to maximize the probabilities of ground-truth class for all data. And the learning of weight is through gradient descent. </p>
<p>To evaluate the performance of monitoring the distance to the audiences, we designed a 40 seconds dance sequence, and a dancer repeated this dance sequence standing at the center of these two regions. The dance sequence performed in region 1 is regarded as “close”, in region 2 is regarded as “medium” and in region 3 is regarded as “far”. </p>
<center>
<img src="image/Pic2.png" width="900px">
<p>Fig. 2: Tested dance sequence for monitoring the relative distance to the audience. </p>
</center>
<p>To evaluate the performance of monitoring the distance between body parts, we designed three movements. In the first movement, the hands are closer than feet; in the second movement, the feet is closer than hands; and in the third movement, their distances are similar. The dancer repeated these three movements in the three regions same as the previous experiment. In each region, each movement is retained for 120 seconds. </p>
<center>
<img src="image/Pic3.png" width="430px">
<p>Fig. 3: Tested movements for monitoring the relative distance between body parts. </p>
</center>
<p>For dancers to edit media, a media authoring tool is provided. The tool will first recommend a series of background images to dancers by learning their preferences for dance style. The recommended images are tabulated on the right-hand panel, as shown in the figure, and the dancers can select from these images. On the left-hand panel, dancers can check the MAC address of the connected beacons and the uploaded background music for their dances. After clicking the "Start
Dance!" button, the system will keep monitoring the dancer's relative distance to the audience and the
relative distance between the dancer's body parts accordingly and the monitoring results will be used to trigger the visuals.<b>The source code can be found here: <a href="https://github.com/Jiqing1107/DanceGUI" target="_blank">Click</a></b>.</p>
<center>
<img src="image/Pic4.png" width="800px">
<p>Fig. 4: The user interface of the media authoring tool. </p>
</center>
<h3>Accelerometer-based Interaction</h3>
<p>In a previous project, we also tried to use an accelerometer to enable the interaction. The accelerometer sensor is put into a glove-shaped cloth, placed on the back of the hand of the dancer. In this project, the system monitors the acceleration of the dancer’s movement, which is an indicator of the movement strength, and trigger the stage effects. The followings are two demos:</p>
<center>
<h5><b>A demo performed by myself:</b></h5>
<iframe 
    width="600" 
    height="350" 
    src="https://www.youtube.com/embed/etoeW2mEuxk"
    frameborder="0" 
    allowfullscreen>
</iframe>
<br />
<br />
<h5><b>Another demo by my partner Qin Chen:</b></h5>
<iframe 
    width="600" 
    height="350" 
    src="https://www.youtube.com/embed/8pme3IkHOcg"
    frameborder="0" 
    allowfullscreen>
</iframe>
</center>
<br />
<br />
</body>
</html>
